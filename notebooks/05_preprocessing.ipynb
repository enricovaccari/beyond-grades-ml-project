{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d0a801",
   "metadata": {},
   "source": [
    "<img src=\"../extra/images/Beyond_Grades_Banner_01.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39de21",
   "metadata": {},
   "source": [
    "# ðŸ“š Project Index\n",
    "\n",
    "- [1 - IMPORTS](#1---imports)\n",
    "- [2 - DATASET LOAD](#2---dataset-load)\n",
    "- [3 - STATISTICAL PREPROCESSING](#6---statistical-preprocessing)\n",
    "  - [3.1 - Encoding categoricals](#61---encoding-categoricals)\n",
    "  - [3.2 - Numeric scaling / normalization](#62---numeric-scaling--normalization)\n",
    "  - [3.3 - Outliers & imputation](#63---outliers--imputation)\n",
    "  - [3.4 - Other steps](#64---other-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebb1ea",
   "metadata": {},
   "source": [
    "---\n",
    "# 1 - IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca5b88",
   "metadata": {},
   "source": [
    "### 1.1 - SETUP PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d91a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning.py functions module imported from: C:\\Users\\Vaccari\\Desktop\\iCloudDrive\\Desktop\\ENRICO\\05_LEARNING\\University\\ToU\\Phases\\02_Calibration_Phase\\Applied_Machine_Learning\\Regression\\beyond-grades-ml-project\\src\\data\n",
      "preprocessing.py functions module imported from: C:\\Users\\Vaccari\\Desktop\\iCloudDrive\\Desktop\\ENRICO\\05_LEARNING\\University\\ToU\\Phases\\02_Calibration_Phase\\Applied_Machine_Learning\\Regression\\beyond-grades-ml-project\\src\\data\n",
      "splitting.py functions module imported from: C:\\Users\\Vaccari\\Desktop\\iCloudDrive\\Desktop\\ENRICO\\05_LEARNING\\University\\ToU\\Phases\\02_Calibration_Phase\\Applied_Machine_Learning\\Regression\\beyond-grades-ml-project\\src\\data\n",
      "analysis.py functions module imported from: C:\\Users\\Vaccari\\Desktop\\iCloudDrive\\Desktop\\ENRICO\\05_LEARNING\\University\\ToU\\Phases\\02_Calibration_Phase\\Applied_Machine_Learning\\Regression\\beyond-grades-ml-project\\src\\features\n",
      "utils.py functions module imported from: C:\\Users\\Vaccari\\Desktop\\iCloudDrive\\Desktop\\ENRICO\\05_LEARNING\\University\\ToU\\Phases\\02_Calibration_Phase\\Applied_Machine_Learning\\Regression\\beyond-grades-ml-project\\src\\utilities\n",
      "Imports ready: pd, np, sns, plt, train_test_split, etc.\n",
      "PROJECT_ROOT: C:\\Users\\Vaccari\\Desktop\\iCloudDrive\\Desktop\\ENRICO\\05_LEARNING\\University\\ToU\\Phases\\02_Calibration_Phase\\Applied_Machine_Learning\\Regression\\beyond-grades-ml-project\n"
     ]
    }
   ],
   "source": [
    "# Centralized setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure PROJECT_PATH is in sys\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "PROJECT_PATH = PROJECT_ROOT / \"src\" / \"project\"\n",
    "\n",
    "if str(PROJECT_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_PATH))\n",
    "\n",
    "# Centralized import\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474e353",
   "metadata": {},
   "source": [
    "---\n",
    "# 2 - DATASET LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a2ba1",
   "metadata": {},
   "source": [
    "### 2.1 - LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796b2a7",
   "metadata": {},
   "source": [
    "Here I am loading back in the X sets (with the addition of the newly engineered features) and y sets (which have not undergone any modifications since the splitting stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0590af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train successfully.\n",
      "y_train loaded successfully.\n",
      "X_test successfully.\n",
      "y_test loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "X_train_path = \"../data/interim/03_X_train_feat_engeneered.xlsx\"\n",
    "y_train_path = \"../data/interim/02_y_train_aftersplit.xlsx\"\n",
    "\n",
    "X_test_path = \"../data/interim/03_X_test_feat_engeneered.xlsx\"\n",
    "y_test_path = \"../data/interim/02_y_test_aftersplit.xlsx\"\n",
    "\n",
    "try:\n",
    "    X_train = utils.load_student_dataset(X_train_path)\n",
    "    print('X_train successfully.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during data loading: {e}')\n",
    "\n",
    "try:\n",
    "    y_train = utils.load_student_dataset(y_train_path)\n",
    "    print('y_train loaded successfully.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during data loading: {e}')\n",
    "\n",
    "try:\n",
    "    X_test = utils.load_student_dataset(X_test_path)\n",
    "    print('X_test successfully.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during data loading: {e}')\n",
    "\n",
    "try:\n",
    "    y_test = utils.load_student_dataset(y_test_path)\n",
    "    print('y_test loaded successfully.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during data loading: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae91306",
   "metadata": {},
   "source": [
    "---\n",
    "# 3 - PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eac8a2",
   "metadata": {},
   "source": [
    "Before proceeding to the modeling stage, it is crucial to ensure that all features are properly prepared.  \n",
    "- **Categorical variables** (like `gender`) must be converted into numerical representations (e.g., through one-hot encoding or similar techniques).  \n",
    "- **Numerical features** (like `StudyTimeWeeky`) generally perform better when standardized to a common scale.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f8381",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 3.1 - PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98961c5",
   "metadata": {},
   "source": [
    "> **Note:**  \n",
    "Although I typically store my functions in dedicated `.py` files within the `/src` directory, the following function is essential for the next steps of this notebook. For convenience and clarity, I will define it directly here, bbut you can also find it in (../src/data/preprocessing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de65dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(numeric_features, categorical_features, k_best=None, model=None):\n",
    "    \"\"\"\n",
    "    Preprocess:\n",
    "      - numeric: median impute + StandardScaler\n",
    "      - categorical: most_frequent impute + OneHotEncoder(ignore unknown)\n",
    "    Then (optional) SelectKBest(f_regression, k=k_best), then model.\n",
    "    CV-safe: tutto viene rifittato per ogni fold.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = LinearRegression()\n",
    "\n",
    "    # Define transformers for preprocessing\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # One-hot encode region\n",
    "    \n",
    "    # Define imputers \n",
    "    numerical_imputer = SimpleImputer(strategy='median')  # Impute with median as numeric features are not normally distributed\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')  # Impute with most frequent\n",
    "\n",
    "    # Build preprocessor\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", numerical_imputer),\n",
    "        (\"scaler\", numerical_transformer),\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", categorical_imputer),\n",
    "        (\"ohe\", categorical_transformer),\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", num_pipe, numeric_features),\n",
    "        (\"cat\", cat_pipe, categorical_features),\n",
    "    ])\n",
    "\n",
    "    selector = SelectKBest(score_func=f_regression, k=k_best) if k_best is not None else \"passthrough\"\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"select\", selector),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbc7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names_after_preprocess(pipe, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Column names after preprocessing (original numeric + expanded OHE categorical).\n",
    "    Call AFTER pipe.fit(...).\n",
    "    \"\"\"\n",
    "    pre = pipe.named_steps[\"preprocessor\"]\n",
    "    ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "    cat_names = ohe.get_feature_names_out(categorical_features)\n",
    "    return list(numeric_features) + list(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c4a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features(pipe, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Returns (all_names, scores, mask, selected_names) after fitting.\n",
    "    If k_best=None, mask is all True and scores are NaN.\n",
    "    \"\"\"\n",
    "    all_names = get_feature_names_after_preprocess(pipe, numeric_features, categorical_features)\n",
    "    selector = pipe.named_steps.get(\"select\", None)\n",
    "\n",
    "    if selector in (None, \"passthrough\"):\n",
    "        scores = np.full(len(all_names), np.nan)\n",
    "        mask = np.ones(len(all_names), dtype=bool)\n",
    "        selected_names = all_names\n",
    "        return all_names, scores, mask, selected_names\n",
    "\n",
    "    scores = selector.scores_\n",
    "    mask = selector.get_support()\n",
    "    selected_names = [n for n, keep in zip(all_names, mask) if keep]\n",
    "    return all_names, scores, mask, selected_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6227ecd",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**How I Built My Preprocessing & Modeling Pipeline**\n",
    "\n",
    "My pipeline is designed to ensure robust, reproducible, and leakage-free preprocessing and modeling. Hereâ€™s how it works and why:\n",
    "\n",
    "**Pipeline Construction**\n",
    "\n",
    "- **Numeric features** (`numeric_features`):  \n",
    "    - Imputed with the median (to handle missing values robustly).\n",
    "    - Standardized using `StandardScaler` (zero mean, unit variance).\n",
    "\n",
    "- **Categorical features** (`categorical_features`):  \n",
    "    - Imputed with the most frequent value.\n",
    "    - One-hot encoded (`OneHotEncoder(handle_unknown='ignore')`).\n",
    "\n",
    "- **Feature selection**:  \n",
    "    - Optionally, I use `SelectKBest(f_regression, k=k_best)` to keep only the most predictive features.\n",
    "    - I can also force-keep domain-relevant features (e.g., `EngagementIndex`) even if not selected by univariate tests.\n",
    "\n",
    "- **Model**:  \n",
    "    - The pipeline can end with any estimator (e.g., `LinearRegression`, `Ridge`, etc.).\n",
    "\n",
    "All these steps are combined using `Pipeline` and `ColumnTransformer` for clean, modular, and scikit-learn-compatible workflows.\n",
    "\n",
    "### My Strategy: Cross-Validation & Final Model\n",
    "\n",
    "#### ðŸ”„ What does cross-validation do?\n",
    "\n",
    "- Splits the training set into folds (e.g., 5 parts).\n",
    "- For each fold:\n",
    "    - Fits preprocessing, feature selection, and model on 4/5 of the data.\n",
    "    - Evaluates on the remaining 1/5.\n",
    "- Returns an average metric (e.g., RMSE, RÂ²) â€” this estimates how well the whole process generalizes.\n",
    "- **Note:** Each fold may select slightly different features and fit a slightly different model.\n",
    "\n",
    "#### ðŸš© Why refit on the full training set?\n",
    "\n",
    "- Once Iâ€™ve chosen the best pipeline and parameters (e.g., `k_best=12`, `Ridge(alpha=1.0)`), I refit the entire pipeline on 100% of the training data.\n",
    "- This uses all available information, making the final model as robust as possible.\n",
    "- Feature selection is now based on the whole training set, giving a definitive list of features.\n",
    "- This final model is what I apply to the hold-out test set (or future data).\n",
    "\n",
    "#### âœ… Standard Workflow\n",
    "\n",
    "1. **Cross-validation**\n",
    "     - Choose hyperparameters, model, number of features.\n",
    "     - Get a realistic estimate of performance.\n",
    "\n",
    "2. **Refit on full training set**\n",
    "     - Build the final, unique model with those parameters.\n",
    "     - Get the definitive feature list.\n",
    "\n",
    "3. **Final test**\n",
    "     - Evaluate this model on the (never-seen) test set (which will need scaling/transformation as well).\n",
    "\n",
    "#### ðŸŒ± In short\n",
    "\n",
    "Refitting on the full training set:\n",
    "- Maximizes use of available data â†’ more robust model.\n",
    "- Freezes the final pipeline (selected features, scaling, encoding, parameters).\n",
    "- Prepares the model for production or final test evaluation.\n",
    "\n",
    "This approach ensures that my modeling process is both statistically sound and ready for real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766b086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Definisci le colonne\\nnumeric_features = [\"Age\", \"StudyTimeWeekly\", \"Absences\", \"FamilyCapitalScore\", \"EngagementIndex\"]\\ncategorical_features = [c for c in X_train.columns if c not in numeric_features]\\n\\n# Crea pipeline (usa LinearRegression di default). Metti k_best=12 se vuoi selezione univariata\\npipe = make_cv_pipeline(numeric_features, categorical_features, k_best=12)\\n\\n# Fit sul train\\npipe.fit(X_train, y_train.squeeze())\\n\\n# Ispeziona nomi e selezione\\nall_names, scores, mask, selected_names = get_selected_features(pipe, numeric_features, categorical_features)\\nprint(\"Selected features:\", selected_names)\\n\\n# (Opzionale) mostra le top per score\\nif not np.isnan(scores).all():\\n    top_idx = np.argsort(scores)[::-1][:10]\\n    print(\"\\nTop 10 by univariate score:\")\\n    for i in top_idx:\\n        print(f\"{all_names[i]:<35} score={scores[i]:.4f}\")\\n\\n# Trasforma X_train per vedere la matrice (solo per ispezione/esportazione)\\nX_train_tx = pipe.named_steps[\"preprocessor\"].transform(X_train)\\nif pipe.named_steps[\"select\"] != \"passthrough\":\\n    X_train_tx = pipe.named_steps[\"select\"].transform(X_train_tx)\\n\\nX_train_df = pd.DataFrame(X_train_tx, columns=selected_names, index=X_train.index)\\nprint(\"\\nShape after preprocess+select:\", X_train_df.shape)\\nprint(X_train_df.head())\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Definisci le colonne\n",
    "numeric_features = [\"Age\", \"StudyTimeWeekly\", \"Absences\", \"FamilyCapitalScore\", \"EngagementIndex\"]\n",
    "categorical_features = [c for c in X_train.columns if c not in numeric_features]\n",
    "\n",
    "# Crea pipeline (usa LinearRegression di default). Metti k_best=12 se vuoi selezione univariata\n",
    "pipe = make_cv_pipeline(numeric_features, categorical_features, k_best=12)\n",
    "\n",
    "# Fit sul train\n",
    "pipe.fit(X_train, y_train.squeeze())\n",
    "\n",
    "# Ispeziona nomi e selezione\n",
    "all_names, scores, mask, selected_names = get_selected_features(pipe, numeric_features, categorical_features)\n",
    "print(\"Selected features:\", selected_names)\n",
    "\n",
    "# (Opzionale) mostra le top per score\n",
    "if not np.isnan(scores).all():\n",
    "    top_idx = np.argsort(scores)[::-1][:10]\n",
    "    print(\"\\nTop 10 by univariate score:\")\n",
    "    for i in top_idx:\n",
    "        print(f\"{all_names[i]:<35} score={scores[i]:.4f}\")\n",
    "\n",
    "# Trasforma X_train per vedere la matrice (solo per ispezione/esportazione)\n",
    "X_train_tx = pipe.named_steps[\"preprocessor\"].transform(X_train)\n",
    "if pipe.named_steps[\"select\"] != \"passthrough\":\n",
    "    X_train_tx = pipe.named_steps[\"select\"].transform(X_train_tx)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_tx, columns=selected_names, index=X_train.index)\n",
    "print(\"\\nShape after preprocess+select:\", X_train_df.shape)\n",
    "print(X_train_df.head())\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
